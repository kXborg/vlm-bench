{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba879ed-86a6-4030-90b8-a18387062b20",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e6af28-2a63-4da2-b29c-848728a97d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e467d4-9c48-4afc-856e-79ef4fe30e56",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e46dc4-9f2b-461d-8cbe-ec6fef79df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b9ec9971f40a79a0c29ba3e442bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = \"AIDC-AI/Ovis2.5-2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e9197-9508-47af-a8f3-9f39131609ec",
   "metadata": {},
   "source": [
    "### Download Image(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbfb02a-97c9-4229-b697-8193f5fea1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  767k  100  767k    0     0   9.9M      0 --:--:-- --:--:-- --:--:--  9.9M\n"
     ]
    }
   ],
   "source": [
    " !curl -O \"https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/TIlymOb86R6_Mez3bpmcB.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcbef909-1d0c-4cfd-bde9-a9ca6eafb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ayush's.ipynb\"   bird.jpg   Ovis2.5-2B.ipynb   TIlymOb86R6_Mez3bpmcB.png\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aef6a6a-6057-4ca5-8205-b378821b913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2831, 2652)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('TIlymOb86R6_Mez3bpmcB.png')\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18e179-233d-473f-94de-92dd8bbee5f0",
   "metadata": {},
   "source": [
    "### Resize the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9a1722-ba25-4d7c-a016-d43ad6fb4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = 0.3\n",
    "\n",
    "new_width = int(image.width * scaling_factor)\n",
    "new_height = int(image.height * scaling_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_image = image.resize((new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df6c10-1c63-4337-9213-1e65aa4fae3e",
   "metadata": {},
   "source": [
    "### Setup Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a345dcd7-c3e9-4ae2-be27-c276c94d63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\", \"image\": resized_image},\n",
    "        {\"type\": \"text\", \"text\": \"Calculate the sum of the numbers in the middle box in figure (c).\"},\n",
    "    ],\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f03f8-6f34-4e4f-9089-c76d55a09992",
   "metadata": {},
   "source": [
    "### Setup Input, Generate, and Decode Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24345f83-b6aa-412c-b2c6-e9da05013b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the numbers in the middle box in figure (c) is 1.0. This is calculated by adding 0.2, 0.5, and 0.3 together: 0.2 + 0.5 + 0.3 = 1.0.\n"
     ]
    }
   ],
   "source": [
    "input_ids, pixel_values, grid_thws = model.preprocess_inputs(\n",
    "    messages=messages,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "input_ids = input_ids.cuda()\n",
    "pixel_values = pixel_values.cuda() if pixel_values is not None else None\n",
    "grid_thws = grid_thws.cuda() if grid_thws is not None else None\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs=input_ids,\n",
    "    pixel_values=pixel_values,\n",
    "    grid_thws=grid_thws,\n",
    "    enable_thinking=False,\n",
    "    enable_thinking_budget=False,\n",
    "    max_new_tokens=3072,\n",
    "    thinking_budget=2048,\n",
    ")\n",
    "\n",
    "response = model.text_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c0adf1-3f16-4932-901d-4f14a1360ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 18 18:33:04 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 Ti     Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   34C    P2              90W / 350W |   8774MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     14423      C   ...iv/miniconda3/envs/ayush/bin/python     8768MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b07856-b8c9-4c69-bac0-4973840872ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ayush Env",
   "language": "python",
   "name": "ayush"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

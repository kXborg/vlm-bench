| Feature                                            | **Moondream 1**                                                                                                                                                         | **Moondream 2**                                                                                                                                           | **Moondream 3 (Preview)**                                                                                                                                                                                                |
| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Parameter count / scale**                        | \~1.6B parameters. (Vision encoder + LM backbone)                                                                                                                       | \~1.86B parameters. Slightly larger. ([Data Science Dojo][1])                                                                                             | 9B total, \~2B active (because of MoE). ([Hugging Face][2])                                                                                                                                                              |
| **Vision encoder**                                 | Based on SigLIP with a ViT-style architecture. Depth \~12 transformer blocks; hidden dim 1152; MLP \~4304 → 1152 etc. (no MoE in vision) (as per the dump you provided) | Builds on same SigLIP vision encoder; similar hidden size etc, but deeper and more refined. ([Data Science Dojo][1])                                      | SigLIP-based vision encoder still, but improved: supports multi-crop channel concatenation for higher resolution images more efficiently. ([Hugging Face][2])                                                            |
| **Language Backbone / LM**                         | Uses Phi-1.5 LM for text side. ([Data Science Dojo][1])                                                                                                                 | Same / initialized from Phi-1.5. Probably similar transformer LM architecture. ([Data Science Dojo][1])                                                   | Larger LM hidden dimension: hidden dim = 2048; uses mixture-of-experts (MoE) FFNs in most layers. ([Hugging Face][2])                                                                                                    |
| **Depth / Layers**                                 | \~12 vision transformer blocks. (Vision side) Language side \~24 transformer blocks? (from your dump)                                                                   | More vision blocks: I believe \~27 vision transformer blocks. LM side similar count. ([Data Science Dojo][1])                                             | 24 total layers: first 4 dense; remaining 20 use MoE FFNs. ([Hugging Face][2])                                                                                                                                           |
| **Hidden dimensions / inner-MLP sizes**            | Vision hidden dim 1152; MLP inner size \~4304. LM side hidden \~2048.                                                                                                   | Similar to 1 for vision; likely a bit optimised.                                                                                                          | Vision + LM hidden dim = 2048 in LM part; MoE FFN inner/gate dim = 1024. ([Hugging Face][2])                                                                                                                             |
| **Mixture-of-Experts (MoE)**                       | No MoE. Dense model.                                                                                                                                                    | No MoE; dense.                                                                                                                                            | Yes: MoE FFNs — 64 experts, with \~8 experts activated per token. ([moondream.ai][3])                                                                                                                                    |
| **Context length (text)**                          | Limited to “typical” lengths — 4K tokens or similar.                                                                                                                    | Same \~4K tokens context in training/inference. ([moondream.ai][3])                                                                                       | Up to **32K token context** (extended) with learned temperature scaling etc. ([moondream.ai][3])                                                                                                                         |
| **Attention / positional improvements**            | Standard transformer attention; positional encodings typical.                                                                                                           | Similar attention + standard positional encodings.                                                                                                        | Multi-headed attention with **learned position- and data-dependent temperature scaling**. Also some optimisations for long context. ([Hugging Face][2])                                                                  |
| **OCR / Document / Visual Reasoning capability**   | Basic – image-captioning, VQA, etc. OCR less strong, less document-querying.                                                                                            | Improved: Moondream2 has better vision-language tasks, more image-text alignment etc. But still limitations in complex OCR, etc. ([Data Science Dojo][1]) | Strong improvements: better OCR, document understanding, charts, counting, and structured output (like JSON) etc. Vision reasoning is a key goal. ([Hugging Face][2])                                                    |
| **Efficiency / Active inference / Resource usage** | More lightweight than 2 and 3; good for limited hardware.                                                                                                               | Designed for edge / resource constrained hardware; small, faster inference than large models. ([Medium][4])                                               | More compute overall, but MoE and “active” parameters mean lower effective compute per token; still trying to keep deployment efficiency. Also optimizations with token efficient vision processing. ([Hugging Face][2]) |
| **Trade-offs or complexity**                       | Simpler model; easier to understand/tune; less capacity.                                                                                                                | More capacity, but still dense → growing compute cost with depth.                                                                                         | More complexity: MoE, longer context, more intricate architecture (vision optimizations, temp scaling etc.), more challenging to fully train / deploy.                                                                   |

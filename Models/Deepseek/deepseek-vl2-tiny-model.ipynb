{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek_vl2 tiny model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "<span style=\"font-size:16px\">This notebook demonstrates multimodal image understanding using the DeepSeek-VL model, a vision-language model capable of generating textual descriptions and insights from images. The workflow combines PyTorch, transformers, and DeepSeek-VL utilities to process images and produce inferences.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"font-size:20px\">Clone the official DeepSeek-VL GitHub repository, move into its directory, and install it in editable mode so we can use it directly in our notebook.</span><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\OpenCV University\\vlm-bench\\Models\\Deepseek\n"
     ]
    }
   ],
   "source": [
    "# %cd C:\\Users\\User\\Desktop\\OpenCV University\\vlm-bench\\Models\\Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/deepseek-ai/DeepSeek-VL.git\n",
    "# %cd DeepSeek-VL\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
      "ERROR: No matching distribution found for triton\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U \"triton-windows<3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and modules\n",
    "<span style=\"font-size:16px\">requests & BytesIO: To fetch images directly from GitHub URLs and load them into memory without saving locally.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# For opening and processing images\n",
    "from PIL import Image\n",
    "\n",
    "# Import mattplotlib to plot image outputs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " # Show plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default colormap to grayscale\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load DeepSeek-VL2 tiny Model\n",
    "<span style=\"font-size:16px\">\n",
    "<b>VLChatProcessor</b>: The processor handles preprocessing of both text and image inputs so the model can understand multimodal data.<br>    \n",
    "Tokenizer: Used to convert text into token IDs suitable for the model.<br> \n",
    "MultiModalityCausalLM: This is the actual deep learning model capable of handling both image and text inputs for inference.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current Version\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\vlm\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add pad token = ['<｜▁pad▁｜>'] to the tokenizer\n",
      "<｜▁pad▁｜>:2\n",
      "Add image token = ['<image>'] to the tokenizer\n",
      "<image>:128815\n",
      "Add grounding-related tokens = ['<|ref|>', '<|/ref|>', '<|det|>', '<|/det|>', '<|grounding|>'] to the tokenizer with input_ids\n",
      "<|ref|>:128816\n",
      "<|/ref|>:128817\n",
      "<|det|>:128818\n",
      "<|/det|>:128819\n",
      "<|grounding|>:128820\n",
      "Add chat tokens = ['<|User|>', '<|Assistant|>'] to the tokenizer with input_ids\n",
      "<|User|>:128821\n",
      "<|Assistant|>:128822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/deepseek-vl2-tiny\"\n",
    "vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Inference Function\n",
    "<span style=\"font-size:16px\">\n",
    "Steps inside the function: <br>\n",
    "1. <b>Load image from GitHub</b> → Downloads the image and converts it to RGB format.<br>\n",
    "2. <b>Prepare conversation format</b> → Adds the user’s prompt along with the image placeholder, following the model’s input format.<br>\n",
    "3. <b>Preprocess inputs with `VLChatProcessor`</b> → Converts both text and image into the proper token/embedding format for the model. <br>\n",
    "4. <b>Generate embeddings </b>→ Uses the multimodal model to create embeddings for the input. <br>\n",
    "5. <b>Run inference</b> → Calls the language model to generate a response based on both the text prompt and the image.<br>\n",
    "6. <b>Decode response</b> → Converts the generated tokens back into human-readable text.<br>\n",
    "7. <b>Output</b> → Prints the image filename, inference time, and the model’s answer.<br>\n",
    "<br>\n",
    "\n",
    "This function run_inference allows us to run DeepSeek-VL multimodal inference on images hosted on GitHub without plotting them.<br>\n",
    "<b>force_batchify=True</b> ensures that even a single image + prompt is converted into a “batch” format, making it compatible with the model and avoiding shape/dimension errors.<br>\n",
    "<b>Output:</b> The model’s response as a string, along with the image filename and inference time.\n",
    "</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case, we’ll ask the model to explain an architecture diagram\n",
    "\n",
    "<span style=\"font-style:16px\">We can now test our inference function by passing:\n",
    "1. A GitHub image URL\n",
    "2. A text prompt asking the model to analyze the image </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|User|>: <image>\n",
      "<|ref|>Explain the image in 100 words<|/ref|>.\n",
      "\n",
      "<|Assistant|>: The image displays a list of popular apps categorized into two sections: \"Popular apps\" and \"Must-have free apps.\" Each app has an icon, name, category, and price. The \"Popular apps\" section includes applications like PDF X: PDF Editor & PDF Reader, Screen recorder - Screen record & Screen capture, Sketchbook Pro, Movie Maker Video Editor, DTS Sound Unbound, HEVC Video Extensions, GST Doctor ITC Matching Software Pro, Movie Maker - Video Editor PRO, PDF Reader Pro - PDF Editor & Converter, and Doc Scan PDF Scanner. The \"Must-have free apps\" section features Instagram, Lively Wallpaper, VLC, Netflix, Adobe Acrobat Reader DC, ChatGPT, Telegram Desktop, Canva, Visual Studio Code, and Microsoft Teams.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"<image>\\n<|ref|>Explain the image in 100 words<|/ref|>.\",\n",
    "        \"images\": [\"icons.png\"],\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation,\n",
    "    images=pil_images,\n",
    "    force_batchify=True,\n",
    "    system_prompt=\"\"\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "# run image encoder to get the image embeddings\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "# run the model to get the response\n",
    "outputs = vl_gpt.language.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/esp32-devkitC-v4-pinout.png\",\n",
    "    \"Explain each pin in this ESP32 board.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image.png\",\n",
    "\"Extract all the text from this image as plain text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analyze Table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/Depict-Data-Studio_Transforming-a-Table_GIF.gif\",\n",
    "    \"Explain what transformation is happening in this data table.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram \n",
    "<span style=\"font-siz:16px\">We are using the `run_inference` function to get a detailed step-by-step explanation of the water cycle from the given image</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/pngtree-natural-phenomena-of-water-cycle-seawater-waves-png-image_6940374.png\",\n",
    "    \"Explain the stages of this process step by step.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image1.jpg\",\n",
    "    \"How many people are present in this image? Return only the number.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image2.jpg\",\n",
    "    \"List the colors of the main objects visible in this image. Answer as a JSON array like: [\\\"red\\\", \\\"blue\\\", \\\"green\\\"].\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left and Right object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image3.jpg\",\n",
    "    \"Describe what is on the left side and what is on the right side of this image.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image4.jpg\",\n",
    "    \"Identify the objects in this image and count them.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/image5.jpg\",\n",
    "    \"Describe the details in this street image.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/molecularformula.png\",\n",
    "    (\n",
    "        \"The image shows a table of chemical compounds and their formulas. \"\n",
    "        \"Extract the compounds and formulas, but represent formulas in LaTeX math format. \"\n",
    "        \"Output should be valid JSON only. Example: \"\n",
    "        \"{\\\"Glucose\\\": \\\"$C_{6}H_{12}O_{6}$\\\", \\\"Butane\\\": \\\"$C_{4}H_{10}$\\\"}.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/Piechartexample11.png\",\n",
    "    \"Explain the insights from this chart.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.shutterstock.com/image-photo/broken-screen-laptop-smashed-3d-600nw-2257814995.jpg\" -O laptop.jpg\n",
    "# Load and display using PIL\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open(\"laptop.jpg\")\n",
    "display(img)\n",
    "prompt = (\n",
    "    \"Carefully examine this image. \"\n",
    "    \"Identify any physical damage, broken connectors, burns, missing parts, or anything abnormal. \"\n",
    "    \"Describe the issue clearly and in detail.\"\n",
    ")\n",
    "\n",
    "run_inference(\n",
    "    image_url=\"https://www.shutterstock.com/image-photo/broken-screen-laptop-smashed-3d-600nw-2257814995.jpg\",\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the image\n",
    "!wget \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/battery-charger-9137425.jpg\" -O battery.jpg\n",
    "\n",
    "# Load and display using PIL\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open(\"battery.jpg\")\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Carefully examine this image. \"\n",
    "    \"Identify any physical damage, broken connectors, burns, missing parts, or anything abnormal. \"\n",
    "    \"Describe the issue clearly and in detail.\"\n",
    ")\n",
    "\n",
    "run_inference(\n",
    "    image_url=\"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/battery-charger-9137425.jpg\",\n",
    "    prompt=prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/istockphoto-490106120-612x612.jpg\"\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)\n",
    "run_inference(\n",
    "    image_url=image_url,\n",
    "    prompt=(\n",
    "        \"Carefully examine this electronic component image. \"\n",
    "        \"Identify any physical damage, broken connectors, burns, missing parts, or anything abnormal. \"\n",
    "        \"Describe the issue clearly and in detail.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://raw.githubusercontent.com/Shilpaknnarayan/Images/main/Burnt-Plug-blog.jpg\"\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)\n",
    "run_inference(\n",
    "    image_url=image_url,\n",
    "    prompt=(\n",
    "        \"Carefully examine this burnt plug image. \"\n",
    "        \"Identify any physical damage, broken connectors, burns, missing parts, or anything abnormal. \"\n",
    "        \"Describe the issue clearly and in detail.\"\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

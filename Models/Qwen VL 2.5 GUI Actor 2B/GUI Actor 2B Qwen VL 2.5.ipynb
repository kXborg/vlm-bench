{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1321f-2bad-4f7a-9ee9-b43aa7db8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda Toolkit and CuDnn to be installed before this.\n",
    "!git clone https://github.com/microsoft/GUI-Actor.git\n",
    "%cd GUI-Actor\n",
    "!pip install -e . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe0397-871f-4c7d-b4b8-2da1f40d1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name_or_path = \"microsoft/GUI-Actor-2B-Qwen2-VL\"\n",
    "data_processor = Qwen2VLProcessor.from_pretrained(model_name_or_path)\n",
    "tokenizer = data_processor.tokenizer\n",
    "model = Qwen2VLForConditionalGenerationWithPointer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5ff3c-669b-41e3-849d-82a26f6fde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare example\n",
    "dataset = load_dataset(\"rootsautomation/ScreenSpot\")[\"test\"]\n",
    "example = dataset[0]\n",
    "print(f\"Intruction: {example['instruction']}\")\n",
    "print(f\"ground-truth action region (x1, y1, x2, y2): {[round(i, 2) for i in example['bbox']]}\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a GUI agent. You are given a task and a screenshot of the screen. You need to perform a series of pyautogui actions to complete the task.\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": example[\"image\"], # PIL.Image.Image or str to path\n",
    "                # \"image_url\": \"https://xxxxx.png\" or \"https://xxxxx.jpg\" or \"file://xxxxx.png\" or \"data:image/png;base64,xxxxxxxx\", will be split by \"base64,\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": example[\"instruction\"]\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0f5c7-c2e5-48b4-9229-1782c9c16ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "pred = inference(conversation, model, tokenizer, data_processor, use_placeholder=True, topk=3)\n",
    "px, py = pred[\"topk_points\"][0]\n",
    "print(f\"Predicted click point: [{round(px, 4)}, {round(py, 4)}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

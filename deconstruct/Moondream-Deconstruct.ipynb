{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb5e03e-a541-4456-87c4-37e39c37231a",
   "metadata": {},
   "source": [
    "## Let's Build a Toy Model\n",
    "We will do the following here:\n",
    " - A 4x4 toy image \n",
    " - Split into 2x2 patches (4 pacthes total)\n",
    " - Flatten each patch to a vector of length 4\n",
    " - Linear projection mapping each 4 dimensional patch to an 8 dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bfe56e-3bf1-40d0-9e70-9469a0cdcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ec831-df04-4c45-b7a7-ee3f6e232a19",
   "metadata": {},
   "source": [
    "### Make a toy image\n",
    "A 4x4 grayscale image of 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c6cf9c-cd5e-440d-ab6e-05691ae79b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    }
   ],
   "source": [
    "img = torch.arange(16).reshape(4,4).float()\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab73282-0a16-49d7-9cad-853f9ac45e1e",
   "metadata": {},
   "source": [
    "### Split into Patches\n",
    "Let's say patch size - 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "761879c6-23f6-4c36-aaa6-00ab196ee10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "patches = img.unfold(0,2,2).unfold(1,2,2)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79455379-d951-46ed-b09b-a463e924175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.],\n",
      "          [ 4.,  5.]],\n",
      "\n",
      "         [[ 2.,  3.],\n",
      "          [ 6.,  7.]]],\n",
      "\n",
      "\n",
      "        [[[ 8.,  9.],\n",
      "          [12., 13.]],\n",
      "\n",
      "         [[10., 11.],\n",
      "          [14., 15.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7dbec7-25ad-479f-86e2-26d621d4a0d0",
   "metadata": {},
   "source": [
    "### Flatten Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a674f2cd-e88b-4eae-a74c-4e347efc0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  4.,  5.],\n",
      "        [ 2.,  3.,  6.,  7.],\n",
      "        [ 8.,  9., 12., 13.],\n",
      "        [10., 11., 14., 15.]])\n"
     ]
    }
   ],
   "source": [
    "patches = patches.contiguous().view(-1,2*2)\n",
    "print(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ed9a3c-f42a-43c1-8f4f-531c8012d6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26402d48-b7c8-4ef8-b2d8-f843221b62ba",
   "metadata": {},
   "source": [
    "### Linear Projection (Patch Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86659fe5-2261-4225-ac3c-b777c8e9db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  torch.Size([4, 8])\n",
      "tensor([[-2.3452, -1.7857,  3.0163, -1.2413,  0.2759, -2.3244,  3.0924, -0.2377],\n",
      "        [-3.3043, -3.2541,  6.2068, -1.6844,  0.7843, -2.8811,  3.2565,  0.3049],\n",
      "        [-6.1814, -7.6593, 15.7786, -3.0138,  2.3094, -4.5511,  3.7487,  1.9329],\n",
      "        [-7.1404, -9.1278, 18.9691, -3.4569,  2.8178, -5.1077,  3.9128,  2.4755]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# In 4 --> out 8\n",
    "linear = nn.Linear(4,8)\n",
    "embedded = linear(patches)\n",
    "print(\"Shape: \", embedded.shape)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad05910-726d-4aec-9d8e-794a19cbc7a6",
   "metadata": {},
   "source": [
    "**Random Weights and Biases for Now**\n",
    "\n",
    "The embedding will be random at start since weights are random. Let's check the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a87ad0c-66a4-4aeb-b1ff-1102c60cdb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0786,  0.0531, -0.0895, -0.3645],\n",
      "        [-0.0107, -0.4378, -0.0383, -0.2474],\n",
      "        [ 0.4826,  0.4910,  0.1220,  0.4997],\n",
      "        [ 0.3217, -0.4536,  0.2054, -0.2951],\n",
      "        [ 0.3103,  0.0790, -0.4601,  0.3250],\n",
      "        [ 0.1173,  0.1120, -0.4532, -0.0544],\n",
      "        [-0.4349, -0.1241,  0.3976,  0.2434],\n",
      "        [ 0.4765, -0.1958,  0.4708, -0.4802]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b976a54a-7722-4349-94be-d275a64243c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.2175,  0.0424, -0.4613, -0.1339,  0.4123, -0.3515,  0.4091,  0.4758],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabd843-9101-43c3-991d-8af31f3707b3",
   "metadata": {},
   "source": [
    "### Add Positional Embedding\n",
    "Just like words in a sentence need order, patches in an image need location info. We’ll add simple positional embeddings (learnable vectors). So that each vector encodes content(pixels) + pposition (where it came from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b4fcaa5-c13a-4d66-bad7-47373cb04454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# learnable positional embeddings for 4 patches, each 8 dim\n",
    "pos_embed = nn.Parameter(torch.zeros(4,8))\n",
    "\n",
    "# Add to pacth embeddings\n",
    "embedded_with_pos = embedded + pos_embed\n",
    "print(embedded_with_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a884a65-2a13-4562-a843-0641eeffe5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3452, -1.7857,  3.0163, -1.2413,  0.2759, -2.3244,  3.0924, -0.2377],\n",
       "        [-3.3043, -3.2541,  6.2068, -1.6844,  0.7843, -2.8811,  3.2565,  0.3049],\n",
       "        [-6.1814, -7.6593, 15.7786, -3.0138,  2.3094, -4.5511,  3.7487,  1.9329],\n",
       "        [-7.1404, -9.1278, 18.9691, -3.4569,  2.8178, -5.1077,  3.9128,  2.4755]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_with_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d34c4-2802-4369-9953-36d733419e99",
   "metadata": {},
   "source": [
    "### Mini Self-Attention Layer\n",
    "Let’s mimic a tiny Transformer attention head on these 4 patch embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69779af5-3985-4d78-bf17-ecdc30ccb7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Embedding size\n",
    "d_model = 8\n",
    "\n",
    "# Query/key dimension\n",
    "d_k = d_model // 2\n",
    "\n",
    "# projection matrices\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# (4, d_k)\n",
    "Q = W_q(embedded_with_pos)\n",
    "# (4, d_k)\n",
    "K = W_k(embedded_with_pos)\n",
    "# (4, d_model)\n",
    "V = W_v(embedded_with_pos)\n",
    "\n",
    "# Attention scores (4,4)\n",
    "scores = Q@K.T/(d_k ** 0.5)\n",
    "attn_weights = scores.softmax(dim=-1)\n",
    "\n",
    "# Weighted sum of values\n",
    "attn_output = attn_weights @ V\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32e2d6a1-8519-48d2-bd43-87e18e42cb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6244e-01, -1.9080e+00,  1.5089e+00,  1.4638e+00,  1.9021e+00,\n",
       "         -9.1543e-01,  5.5643e-02,  1.8941e+00],\n",
       "        [ 6.4958e-02, -2.3306e+00,  1.4405e+00,  1.9354e+00,  2.1489e+00,\n",
       "         -9.6926e-01,  4.9013e-02,  2.3018e+00],\n",
       "        [ 1.5942e+00, -5.1724e+00,  9.8078e-01,  5.1069e+00,  3.8086e+00,\n",
       "         -1.3313e+00,  4.4255e-03,  5.0439e+00],\n",
       "        [ 1.8317e+00, -5.6138e+00,  9.0936e-01,  5.5996e+00,  4.0664e+00,\n",
       "         -1.3875e+00, -2.5008e-03,  5.4699e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cca725-4678-4c3a-a41f-130b0371319e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
